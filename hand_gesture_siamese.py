import os
import sys
import tqdm
import yaml

# del sys.modules['hand_gesture_functions']

from hand_gesture_siamese_functions import *

checkpointer = ModelCheckpoint(filepath='siamese_resnet18_weights.hdf5', verbose=1, save_best_only=True)
lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-6)
# early_stopper = EarlyStopping(min_delta=0.0005, patience=20)

# INFO
batch_size = 32
n_classes = 6
n_epochs = 1000

normalize = True
ravel_x = False
one_hot_y = False
network_type = "resnet18"

if ravel_x:
    input_shape = (60*60*3,)
else:
    input_shape = (60, 60, 3)

data_augmentation = True

# Data
X_train, y_train, _ = make_data("train", normalize=normalize, ravel_x=ravel_x, one_hot_y=one_hot_y)
X_val, y_val, _ = make_data("val", normalize=normalize, ravel_x=ravel_x, one_hot_y=one_hot_y)
X_train = np.vstack((X_train, X_val))
y_train = np.append(y_train, y_val)
X_test, y_test, _ = make_data("test", normalize=normalize, ravel_x=ravel_x, one_hot_y=one_hot_y)

# Model
model = make_siamese_model(input_shape, network_type=network_type)

# Compile
rms = RMSprop()
adam = Adam()
model.compile(loss=contrastive_loss, optimizer='adam', metrics=[accuracy])

print("Loading weights...")
if "siamese_resnet18_weights.hdf5" in os.listdir('.'):
    model.load_weights("siamese_resnet18_weights.hdf5")

# Callback
plot_loss_and_acc = PlotLossAndAccSiamese()

try:
    # Fit
    if not data_augmentation:
        print('Not using data augmentation.')

        # create training+test positive and negative pairs
        X_train_pairs, y_train_pairs = make_pairs(X_train, y_train)
        X_test_pairs, y_test_pairs = make_pairs(X_test, y_test)

        # Fit
        model.fit([X_train_pairs[:, 0], X_train_pairs[:, 1]], y_train_pairs,
              batch_size=batch_size,
              epochs=n_epochs,
              validation_data=([X_test_pairs[:, 0], X_test_pairs[:, 1]], y_test_pairs),
              shuffle=True,
              callbacks=[lr_reducer, early_stopper, plot_loss_and_acc])

    else:
        print('Using real-time data augmentation.')
        # This will do preprocessing and realtime data augmentation:
        
        train_data_gen = gen_shuffled_augmented_train_pairs_data(X_train, y_train, batch_size)
        X_train_pairs, y_train_pairs = make_pairs(X_train, y_train) # Dummy
        X_test_pairs, y_test_pairs = make_pairs(X_test, y_test)

        # print("BEFORE FIT: Memory usage: %s (kb)" % resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)

        # Fit the model on the batches generated by datagen.flow().
        model.fit_generator(train_data_gen,
                            steps_per_epoch=len(y_train_pairs)//batch_size,
                            max_queue_size=100, verbose=1,
                            validation_data=([X_test_pairs[:, 0], X_test_pairs[:, 1]], y_test_pairs), epochs=n_epochs,
                            callbacks=[checkpointer, lr_reducer, plot_loss_and_acc],
                            pickle_safe=False)
except KeyboardInterrupt:
    pass

# compute final accuracy on training and test sets
print("Predicting...")
y_pred = model.predict([X_train_pairs[:, 0], X_train_pairs[:, 1]])
tr_acc = compute_accuracy(y_train_pairs, y_pred)
y_pred = model.predict([X_test_pairs[:, 0], X_test_pairs[:, 1]])
te_acc = compute_accuracy(y_test_pairs, y_pred)

print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))
print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))

'''
##########################################################
# Load weights
##########################################################

model.load_weights('siamese_resnet34_weights.hdf5')

##########################################################
# TEST
##########################################################

y_test_preds = []
test_on = 120
for x_test in tqdm.tqdm(X_test):
    y_preds = []
    for theClass in range(6):
        X_for_test_pred = [ np.array([x_test]*test_on), X_train[y_train==theClass][:test_on] ]
        y_preds.append(np.mean(model.predict(X_for_test_pred)))
        # print(y_preds)
    y_test_preds.append(np.argmin(y_preds))


##########################################################
# VAL
##########################################################

y_test_preds = []

X_val, y_val, file_names = make_data("val", normalize=normalize, ravel_x=ravel_x, one_hot_y=one_hot_y)

file_numbers = [int(f.split('/')[-1].split('.')[0]) for f in file_names]
sorted_order = np.argsort(file_numbers)
X_val = X_val[sorted_order]
y_val = y_val[sorted_order]
file_names = np.array(file_names)[sorted_order]

y_val_preds = []
test_on = 100
for x_val in tqdm.tqdm(X_val):
    y_preds = []
    for theClass in range(6):
        X_for_val_pred = [ np.array([x_val]*test_on), X_train[y_train==theClass][np.random.choice(np.arange(np.sum(y_train==theClass)), test_on, replace=False)] ]
        y_preds.append(np.mean(model.predict(X_for_val_pred)))
        # print(y_preds)
    y_val_preds.append(np.argmin(y_preds))

results = []
for i in range(len(y_val_preds)):
    results.append([file_names[i].split('/')[-1], num_to_class(y_val_preds[i])])

write_to_csv(results)

'''

test_data = load_csv_data(["labels_test_test.csv"])

file_names = ["test_data/"+i[0] for i in test_data]

X_test_data = []
for file in file_names:
    image = cv2.imread(file)
    im60 = cv2.resize(image, (60, 60))
    if normalize:
            im60 = im60 / 255.
    if ravel_x:
        X_test_data.append(im60.ravel())
    else:
        X_test_data.append(im60)

X_test_data = np.array(X_test_data)


test_on = 400
compare = []
for theClass in range(6):
    compare.append(X_train[y_train==theClass][np.random.choice(np.arange(np.sum(y_train==theClass)), test_on, replace=False)])

y_distances = []
for x_test_data in tqdm.tqdm(X_test_data):
    y_distances.append([])
    for theClass in range(6):
        X_for_test_data_pred = [ np.array([x_test_data]*test_on), compare[theClass] ]
        y_distances[-1].append(model.predict(X_for_test_data_pred))
        # print(y_preds)

# y_test_data_preds.append(np.argmin(y_preds) // test_on)

y_test_data_preds = np.zeros((len(X_test_data)), dtype=int)
top = 20
for i in range(len(X_test_data)):
    y_test_data_pred = np.mean(np.array(y_distances[i]), axis=1)
    y_test_data_preds[i] = np.argmin(y_test_data_pred)

    sorted_order = np.argsort(y_test_data_pred)[::-1]
    sorted_preds = y_test_data_pred[sorted_order]
    y_test_data_preds[i] = int(np.argmax(np.bincount(sorted_order[:top]//test_on)))


results = []
for i in range(len(y_test_data_preds)):
    results.append([file_names[i].split('/')[-1], num_to_class(y_test_data_preds[i])])

write_to_csv(results)

